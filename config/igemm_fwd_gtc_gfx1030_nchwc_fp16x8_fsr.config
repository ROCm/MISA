[codegen]
arch = 'gfx1030'
code_object = 'cov3'
mode = 'flat'

#########################################################################################
#--------------------------- 16x512x256
[igemm_fwd_gtc]
gemm_m_per_block         = 16
gemm_n_per_block         = 512
gemm_k_per_block         = 256
lanegroup_tile_m         = 8
lanegroup_wave_m         = 1
lanegroup_repeat_m       = 2
lanegroup_tile_n         = 8
lanegroup_wave_n         = 8
lanegroup_repeat_n       = 1
tensor_a_thread_lengths  = [1, 1, 1,  8]       # 1xCEx1xK/Vec-c
tensor_a_cluster_lengths = [1,32, 1, 16]       # 1xCEx1xK
tensor_b_thread_lengths  = [1, 1, 1,  8]       # 1xCExNB0xVec-c
tensor_b_cluster_lengths = [1, 1, 1,512]       # 1xCEx1xNB1
direction                = "fwd"
precision                = "fp16"
tensor_layout            = 'nchwc_cyxkc'
nxb                      = 0
nxe                      = 1
wavefront_size           = 64
cumode                   = 0
vector_c                 = 8
mini_weights             = 1
tensor_b_pass_through    = 1

#--------------------------- 16x256x128
[igemm_fwd_gtc]
gemm_m_per_block         = 16
gemm_n_per_block         = 256
gemm_k_per_block         = 128
lanegroup_tile_m         = 8
lanegroup_wave_m         = 1
lanegroup_repeat_m       = 2
lanegroup_tile_n         = 8
lanegroup_wave_n         = 8
lanegroup_repeat_n       = 1
tensor_a_thread_lengths  = [1, 1, 1,  8]       # 1xCEx1xK/Vec-c
tensor_a_cluster_lengths = [1,16, 1, 16]       # 1xCEx1xK
tensor_b_thread_lengths  = [1, 1, 1,  8]       # 1xCExNB0xVec-c
tensor_b_cluster_lengths = [1, 1, 1,256]       # 1xCEx1xNB1
direction                = "fwd"
precision                = "fp16"
tensor_layout            = 'nchwc_cyxkc'
nxb                      = 0
nxe                      = 1
wavefront_size           = 32
cumode                   = 0
vector_c                 = 8
mini_weights             = 1
tensor_b_pass_through    = 1

#--------------------------- 16x64x32
[igemm_fwd_gtc]
gemm_m_per_block         = 16
gemm_n_per_block         = 64
gemm_k_per_block         = 32
lanegroup_tile_m         = 8
lanegroup_wave_m         = 1
lanegroup_repeat_m       = 2
lanegroup_tile_n         = 8
lanegroup_wave_n         = 8
lanegroup_repeat_n       = 1
tensor_a_thread_lengths  = [1, 1, 1,  8]       # 1xCEx1xK/Vec-c
tensor_a_cluster_lengths = [1, 4, 1, 16]       # 1xCEx1xK
tensor_b_thread_lengths  = [1, 1, 1,  8]       # 1xCExNB0xVec-c
tensor_b_cluster_lengths = [1, 1, 1, 64]       # 1xCEx1xNB1
direction                = "fwd"
precision                = "fp16"
tensor_layout            = 'nchwc_cyxkc'
nxb                      = 0
nxe                      = 1
wavefront_size           = 64
cumode                   = 0
vector_c                 = 8
mini_weights             = 1
tensor_b_pass_through    = 1
